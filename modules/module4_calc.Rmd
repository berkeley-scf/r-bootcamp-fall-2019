% R bootcamp, Module 4: Calculations
% August 2019, UC Berkeley
% Chris Paciorek

```{r chunksetup, include=FALSE} 
# include any code here you don't want to show up in the document,
# e.g. package and dataset loading
library(foreign)
library(fields)
if(!('modules' %in% unlist(strsplit(getwd(), split = '/')))) setwd('modules')
gap <- read.csv(file.path('..', 'data', 'gapminder-FiveYearData.csv'), stringsAsFactors = FALSE)
gap2007 <- gap[gap$year == 2007, ]
```

# Reminder: Vectorized calculations and comparisons

At the core of R is the idea of doing calculations on entire vectors.

```{r}
gdpTotal <- gap$gdpPercap * gap$pop

gdpSubset <- gap2007$gdpPercap[1:10]

gdpSubset >= 30000

vec1 <- rnorm(5)
vec2 <- rnorm(5)
vec1 > vec2

vec1 == vec2
vec1 != vec2
## careful: 
vec1 = vec2
identical(vec1, vec2)

## using 'or'
gdpSubset >= 100000 | gdpSubset <= 1000
## using 'and'
gap$gdpPercap[1:10] >= 100000 & gap$continent[1:10] == "Asia"
```

An important related concept is that of recycling
```{r}
vec10 <- sample(1:10, 10, replace = TRUE)
vec3 <- sample(1:10, 3, replace = TRUE)
vec5 <- sample(1:10, 5, replace = TRUE)
vec10
vec3
vec5

vec10 + vec5
vec10 + vec3
```

**Question**: Tell me what's going on. What choices were made by the R developers?

# Vectorized calculations

As we've seen, R has many functions that allow you to operate on each element of a vector all at once.

```{r}
vals <- rnorm(1000)
chi2vals <- vals^2
chi2_df1000 <- sum(chi2vals)
# imagine if the code above were a loop, or three separate loops
```

Advantages:

* much faster than looping
* easier to code
* easier to read and understand the code

Sometimes there are surprises in terms of what is fast, as well as tricks for vectorizing things in unexpected ways:
```{r}
vals <- rnorm(1e6)
system.time(trunc <- ifelse(vals > 0, vals, 0))
system.time(vals <- vals * (vals > 0))
```

**Question**: What am I doing with `vals * (vals > 0)` ? What happens behind the scenes in R?

If you use a trick like this, having a comment in your code is a good idea.

Lots of functions in R are vectorized, such as some we've already used.

```{r}
tmp <- as.character(gap$year)
gap$year2 <- substring(tmp, 3, 4)
head(gap$year2)
```

Question: Note the code above runs and the syntax is perfectly good R syntax, but in terms of what it does, there is a bug in it. See if you can see what it is.

# Linear algebra 

R can do essentially any linear algebra you need. It uses system-level packages called BLAS (basic linear algebra subroutines) and LAPACK (linear algebra package). Note that these calculations will be essentially as fast as if you wrote C code because R just calls C and Fortran routines to do the calculations.

The BLAS that comes with R is fairly slow. It's possible to use a faster BLAS, as well as one that uses multiple cores automatically. This can in some cases give you an order of magnitude speedup if your work involves a lot of matrix manipulations/linear algebra. More details in Module 10.


# Vectorized vector/matrix calculations

Recall that `+`, `-`,`*`, `/` do vectorized calculations:

```{r}
A <- matrix(1:9, 3)
B <- matrix(seq(4,36, by = 4), 3)

A + B
A + B[ , 1]
A * B
A * B[ , 1]
```

Matrix/vector multiplication

```{r}
A %*% B[ , 1]
A %*% B

identical(t(A)%*%A, crossprod(A))
```

Some decompositions

```{r cache=TRUE}
## next 3 lines generate a positive definite matrix
library(fields)
times <- seq(0, 1, length = 100)
R <- exp(-rdist(times) / 0.2) # a correlation matrix
######################################################
e <- eigen(R)
range(e$values)
e$vectors[ , 1]

sv <- svd(R)
U <- chol(R)

devs <- rnorm(100)
Rinvb <- solve(R, devs)  # R^{-1} b
Rinv <- solve(R) # R^{-1} -- try to avoid this
```


# Pre-allocation

This is slow.
```{r cache=TRUE}
vals <- 0
n <- 50000
system.time({
for(i in 1:n)
      vals <- c(vals, i)
})
```

The same holds for using `rbind()`, `cbind()`, or adding to a list, one element at a time.


**Question**: Thoughts on why this are so slow? Think about what R might be doing behind the scenes

# The answer is to pre-allocate memory

This is not so slow. (Please ignore the for-loop hypocrisy and the fact that I could do this as `vals <- 1:n`.)

```{r}
n <- 50000
system.time({
vals <- rep(0, n)
# alternatively: vals <- as.numeric(NA); length(vals) <- n
for(i in 1:n)
      vals[i] <- i
})
```

Here's how to pre-allocate an empty list: 
```{r}
vals <- list(); length(vals) <- n
head(vals)
```

# apply

Some functions aren't vectorized, or you may want to use a function on every row or column of a matrix/data frame, every element of a list, etc.

For this we use the `apply()` family of functions.

```{r}
mat <- matrix(rnorm(100*1000), nr = 100)
row_min <- apply(mat, MARGIN = 1, FUN = min)
col_max <- apply(mat, MARGIN = 2, FUN = max)
```

There are actually some even faster specialized functions:
```{r}
row_mean <- rowMeans(mat)
col_sum <- colSums(mat)
```

# `lapply()` and `sapply()`

```{r}
myList <- list(rnorm(3), rnorm(3), rnorm(5))
lapply(myList, min)
sapply(myList, min)
```

Note that we don't generally want to use `apply()` on a data frame. 

You can use `lapply()` and `sapply()` on regular vectors, such as vectors of indices, which can come in handy, though this is a silly example:
```{r}
sapply(1:10, function(x) x^2)
```


# And more `apply()`s

There are a bunch of `apply()` variants, as well as parallelized versions of them:

* `tapply()`, `vapply()`, `mapply()`, `rapply()`, `eapply()`
* for parallelized versions see Module 10 or `?clusterApply`

# Tabulation 

- Sometimes we need to do some basic checking for the number of observations or types of observations in our dataset
- To do this quickly and easily, `table()` is our friend

```{r table}
tbl <- table(gap$country, gap$continent)
tbl
rowSums(tbl)
all(rowSums(tbl) == 12)
```


# Discretization

You may need to discretize a continuous variable [or a discrete variable with many levels], e.g., by life expectancy:
```{r fig.width=9}
gap2007$lifeExpBin <- cut(gap2007$lifeExp, breaks = c(0, 40, 50, 60, 70, 75, 80, Inf))
tbl <- table(gap2007$continent, gap2007$lifeExpBin)
round( prop.table(tbl, margin = 1), 2 )
```

# Stratified analyses I
Often we want to do individual analyses within subsets or clusters of our data.

As a first step, we might want to just split our dataset by a stratifying variable.  

```{r, fig.cap ="", fig.width=12}
subsets <- split(gap,  gap$year)
length(subsets)
dim(subsets[['2007']])
par(mfrow = c(1,2))
plot(lifeExp ~ gdpPercap, data = subsets[['1952']], main = '1952')
abline(h = 0, col = 'grey')
plot(lifeExp ~ gdpPercap, data = subsets[['2007']], main = '2007')
abline(h = 0, col = 'grey')
```

Obviously, we'd want to iterate to improve that plot given the outlier.

# Stratified analyses II

Often we want to do individual analyses within subsets or clusters of our data. R has a variety of tools for this; for now we'll look at `aggregate()` and `by()`. These are wrappers of `tapply()`. 

```{r aggregate1}
gmSmall <- gap[ , c('lifeExp', 'gdpPercap')]  # reduce to only numeric columns
aggregate(gmSmall, by = list(year = gap$year), FUN = median, na.rm = TRUE) # na.rm not needed here but illustrates use of additional arguments to FUN
aggregate(lifeExp ~ year + continent, data = gap, FUN = median)
agg <- aggregate(lifeExp ~ year + continent , data = gap, FUN = median)
xtabs(lifeExp ~ ., data = agg)
```

Notice the 'long' vs. 'wide' formats. You'll see more about that sort of thing in Module 6.


# Stratified analyses III

`aggregate()` works fine when the output is univariate, but what about more complicated analyses than computing the median, such as fitting a set of regressions?

```{r}
out <- by(gap, gap$year, 
    function(sub) {
      lm(lifeExp ~ log(gdpPercap), data = sub)
    }          
)
length(out)
summary(out[['2007']])
summary(out[['1952']])
```

# Sorting

`sort()` applied to a vector does what you expect.

Sorting a matrix or dataframe based on one or more columns is a somewhat manual process, but once you get the hang of it, it's not bad.

```{r}
ord <- order(gap$year, gap$lifeExp, decreasing = TRUE)
ord[1:5]
gm_ord <- gap[ord, ]
head(gm_ord)
```

You could of course write your own *sort* function that uses `order()`. More in Module 6.

# Merging Data

We often need to combine data across multiple data frames, merging on common fields (i.e., *keys*). In database terminology, this is a *join* operation.

Suppose that our dataset did not have 'continent' in it, but that we had a separate data frame that matches country to continent.

```{r} 
# ignore the 'wizard' behind the curtain...
c2c <- unique(gap[ , c('country', 'continent')])
gapSave <- gap
gap <- gap[ , -which(names(gap) == "continent")]
```

Now let's add the continent info in:

```{r}
head(c2c)
head(gap)

gap <- merge(gap, c2c, by.x = 'country', by.y = 'country',
                   all.x = TRUE, all.y = FALSE)

dim(gapSave)
dim(gap)
identical(gapSave, gap)
identical(gapSave, gap[ , names(gapSave)])
```

What's the deal with the `all.x` and `all.y` ?  We can tell R whether we want to keep all of the `x` observations, all the `y` observations, or neither, or both, when there may be rows in either of the datasets that don't match the other dataset.

# Breakout

### Basics

1) Create a vector that concatenates the country and year to create a 'country-year' variable in a vectorized way using the string processing functions.

2) Use `table()` to figure out the number of countries available for each continent.

### Using the ideas

3) Explain the steps of what this code is doing: `tmp <- gap[ , -which(names(gap) == "continent")]`.

4) Compute the number of NAs in each column of the gapminder dataset using `sapply()` and making use of the `is.na()` function.

5) Discretize gdpPercap into some bins and create a gdpPercap_binned variable. Count the number of values in each bin.

6) Create a boxplot of life expectancy by binned gdpPercap.

7) Sort the dataset and find the shortest life expectancy value. Now consider the use of `which.min()` and why using that should be much quicker with large datasets.
 
8) Create a dataframe that has the total population across all the countries for each year.

9) Merge the info from problem 8 back into the original gapminder dataset. Now plot life expectancy as a function of world population. 

10)  Suppose we have two categorical variables and we conduct a hypothesis test of independence. The chi-square statistic is: 

$$
\chi^2 = \sum_{i=1}^{n}\sum_{j=1}^{m} \frac{(y_{ij} - e_{ij})^2}{e_{ij}}, 
$$ 

where $e_{ij} = \frac{y_{i\cdot} y_{\cdot j}}{y_{\cdot \cdot}}$, with $y_{i\cdot}$ the sum of the values in the i'th row, $y_{\cdot j}$ the sum of values in the j'th column, and $y_{\cdot\cdot}$ the sum of all the values. Suppose I give you a matrix in R with the $y_{ij}$ values. 

You can generate a test matrix as: 
```{r, eval=FALSE}
y <- matrix(sample(1:10, 12, replace = TRUE), 
nrow = 3, ncol = 4)
```

Compute the statistic without *any* loops as follows:

a. First, assume you have the *e* matrix. How do you compute the statistic without loops as a function of `y` and `e`?
b. How can you construct the *e* matrix? Hint: the numerator of *e* is just an *outer product* for which the `outer()` function can be used.

### Advanced 

11) For each combination of year and continent, find the 95th percentile of life expectancy. 

12) Here's a cool trick to pull off a particular element of a list of lists:

```{r}
params <- list(a = list(mn = 7, sd = 3), b = list(mn = 6,sd = 1), 
  c = list(mn = 2, sd = 1))
sapply(params, "[[", 1)
```

Explain what that does and why it works.

Hint:
```{r}
test <- list(5, 7, 3)
test[[2]]
# `[[`(test, 2)  # need it commented or R Markdown processing messes it up...

# `+`(3, 7)
```
